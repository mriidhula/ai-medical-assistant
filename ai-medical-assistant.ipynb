{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionLandmarkNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, base_channels=32, expansion_factor=2):\n",
    "        super(EmotionLandmarkNet, self).__init__()\n",
    "        self.base_channels = base_channels\n",
    "        self.expansion_factor = expansion_factor\n",
    "        sequence_length = input_size // 3\n",
    "\n",
    "        def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.SiLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def separable_conv(in_channels, out_channels, stride=1):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
    "                nn.BatchNorm1d(in_channels),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.prep = conv_block(3, self.base_channels, kernel_size=5, stride=2, padding=2)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            separable_conv(self.base_channels, self.base_channels * self.expansion_factor, stride=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            separable_conv(self.base_channels * self.expansion_factor, self.base_channels * self.expansion_factor, stride=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            separable_conv(self.base_channels * self.expansion_factor, self.base_channels * 2 * self.expansion_factor, stride=2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            separable_conv(self.base_channels * 2 * self.expansion_factor, self.base_channels * 2 * self.expansion_factor, stride=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            separable_conv(self.base_channels * 2 * self.expansion_factor, self.base_channels * 4 * self.expansion_factor, stride=2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            separable_conv(self.base_channels * 4 * self.expansion_factor, self.base_channels * 4 * self.expansion_factor, stride=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.6) # Increased dropout here\n",
    "        self.fc = nn.Linear(1, num_classes) # Placeholder, will be updated\n",
    "\n",
    "        # --- DEBUGGING FORWARD PASS ---\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, input_size)\n",
    "            x = dummy_input.view(1, 3, -1)\n",
    "            x = self.prep(x)\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            self._flattened_size = x.flatten().shape[0]\n",
    "            print(f\"Calculated flattened size: {self._flattened_size}\")\n",
    "            # Update the FC layer with the correct input size\n",
    "            self.fc = nn.Linear(self._flattened_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, 3, -1)\n",
    "\n",
    "        x = self.prep(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Preprocessing ---\n",
    "class WoundDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_paths.append(os.path.join(class_dir, filename))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, os.path.basename(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "input_size = 224 * 224 * 3 # Example: flatten image of 224x224x3\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "validation_split = 0.2\n",
    "\n",
    "# --- Data Transformations ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Dataset and Split ---\n",
    "wound_data_dir = 'wound_data'\n",
    "dataset = WoundDataset(root_dir=wound_data_dir, transform=transform)\n",
    "train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=validation_split, stratify=dataset.labels, random_state=42)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Model, Loss, and Optimizer ---\n",
    "model = EmotionLandmarkNet(input_size=input_size, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001) # Added weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for i, (inputs, labels, _) in progress_bar:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total_samples\n",
    "        train_accuracy = correct_predictions / total_samples\n",
    "        progress_bar.set_postfix(loss=train_loss, accuracy=train_accuracy)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # --- Validation Loop ---\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_samples += labels.size(0)\n",
    "\n",
    "    val_loss = val_running_loss / val_total_samples\n",
    "    val_accuracy = val_correct_predictions / val_total_samples\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Training and Validation Metrics ---\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_accuracies, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'ro-', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- After the training loop ---\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "model_save_path = 'wound_classification_model_v2.pth'\n",
    "\n",
    "# Save the state dictionary of the trained model\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"\\nModel saved successfully to: {model_save_path}\")\n",
    "\n",
    "# --- Later, to load the model ---\n",
    "# Create an instance of your model architecture\n",
    "loaded_model = EmotionLandmarkNet(input_size=input_size, num_classes=num_classes)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Set the model to evaluation mode if you intend to use it for inference\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prediction Function ---\n",
    "def predict_image(image_path, model, transform, class_names, device):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, 1)\n",
    "        predicted_class_name = class_names[predicted_class.item()]\n",
    "    return confidence.item(), predicted_class_name, os.path.splitext(os.path.basename(image_path))[0].split('_')[0] # Remove numbers and extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from pinecone import Pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "PINECONE_API_KEY = \"pcsk_25mGEG_38SLB2BbJDvvtkHN2V8NCK2MP9yyyFnm5e7U9tGB7SnJoJVjAbVnpR6nYtDn9qA\"\n",
    "\n",
    "# Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob=\"*.pdf\",\n",
    "                             loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf(\"data/\")\n",
    "\n",
    "# Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))\n",
    "\n",
    "# Download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "import sentence_transformers\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"medical-assistant\"\n",
    "\n",
    "try:\n",
    "    pc.describe_index(index_name)\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "except pinecone.exceptions.NotFoundException:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "    print(f\"Index '{index_name}' created successfully.\")\n",
    "\n",
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "try:\n",
    "    docsearch = PineconeVectorStore.from_existing_index(\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    print(f\"Successfully loaded existing index '{index_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading existing index: {e}. Creating new index.\")\n",
    "    docsearch = PineconeVectorStore.from_documents(\n",
    "        documents=text_chunks,\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    print(f\"New index '{index_name}' created and populated.\")\n",
    "\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "print(\"Retrieved documents:\", retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "# Load your local LLM model\n",
    "def load_local_llm(model_path):\n",
    "    llm = CTransformers(\n",
    "        model=model_path,\n",
    "        model_type=\"llama\",\n",
    "        config={'max_new_tokens': 256, 'temperature': 0.0}\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm_model_path = \"model/llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "llm = load_local_llm(llm_model_path)\n",
    "\n",
    "# Create the RAG chain\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If the user mentions that the situation is critical or an emergency, \"\n",
    "    \"list out remedies in simple steps that the user should take to deal with the emergency. \"\n",
    "    \"Otherwise, provide a normal answer. In all cases, format the answer neatly and clearly.\"\n",
    "    \"If you don't know the answer, say that you don't know.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "questions_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, questions_answer_chain)\n",
    "\n",
    "# Invoke the RAG chain\n",
    "query = \"What is Acne?\"\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"RAG Response:\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your local LLM model\n",
    "def load_local_llm(model_path):\n",
    "    llm = CTransformers(\n",
    "        model=model_path,\n",
    "        model_type=\"llama\",\n",
    "        config={'max_new_tokens': 500, 'temperature': 0.2}\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm_model_path = \"model/llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "llm = load_local_llm(llm_model_path)\n",
    "\n",
    "# Create the RAG chain\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If the user mentions that the situation is critical or an emergency, \"\n",
    "    \"list out remedies in simple steps that the user should take to deal with the emergency. \"\n",
    "    \"Otherwise, provide a normal answer. In all cases, format the answer neatly and clearly.\"\n",
    "    \"If you don't know the answer, say that you don't know.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "questions_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, questions_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1a: Text to Speech-TTS-model (with gTTS)\n",
    "\n",
    "import os\n",
    "from gtts import gTTS\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "def text_to_speech_with_gtts(input_text, output_filepath):\n",
    "    language=\"en\"\n",
    "\n",
    "    audioobj=gTTS(\n",
    "        text=input_text,\n",
    "        lang=language,\n",
    "        slow=False\n",
    "    )\n",
    "    audioobj.save(output_filepath)\n",
    "\n",
    "    os_name = platform.system()\n",
    "    try:\n",
    "        if os_name == \"Darwin\":\n",
    "            subprocess.run(['afplay', output_filepath])\n",
    "        elif os_name == \"Windows\":\n",
    "            subprocess.run(['ffplay', '-nodisp', '-autoexit', output_filepath],\n",
    "                           stdout=subprocess.DEVNULL,\n",
    "                           stderr=subprocess.DEVNULL\n",
    "                           )\n",
    "        elif os_name == \"Linux\":\n",
    "            subprocess.run(['aplay', output_filepath])\n",
    "        else:\n",
    "            raise OSError(\"Unsupported operating system\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occured while trying to play the audio: {e}\")\n",
    "\n",
    "input_text=\"Hi this is AI with Anurag, autoplay testing\"\n",
    "# text_to_speech_with_gtts(input_text=input_text, output_filepath=\"gtts_testing_autoplay.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Assume you have your EmotionLandmarkNet class defined in this script\n",
    "\n",
    "# Load your trained model (replace with your actual parameters)\n",
    "input_size = 224 * 224 * 3\n",
    "num_classes = 2\n",
    "model = EmotionLandmarkNet(input_size=input_size, num_classes=num_classes)\n",
    "model_path = 'wound_classification_model.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model not found at {model_path}\")\n",
    "    exit()\n",
    "\n",
    "# Define your class names\n",
    "class_names = ['critical', 'non_critical']\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def clean_filename(filename):\n",
    "    \"\"\"Removes numbers and special characters from a filename base.\"\"\"\n",
    "    cleaned_name = re.sub(r'[^a-zA-Z_]', '', filename)\n",
    "    return cleaned_name\n",
    "\n",
    "def predict_single_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            confidence, predicted_class = torch.max(probabilities, 1)\n",
    "            \n",
    "            # Check confidence score threshold\n",
    "            confidence_score = confidence.item()\n",
    "            if confidence_score < 0.7:  # Threshold set to 70%\n",
    "                # Flip to the other class\n",
    "                predicted_class = 1 - predicted_class\n",
    "                confidence_score = 1.0 - confidence_score\n",
    "            \n",
    "            predicted_class_name = class_names[predicted_class.item()]\n",
    "        \n",
    "        filename_base = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        cleaned_filename = clean_filename(filename_base)\n",
    "        return confidence_score, predicted_class_name, cleaned_filename, image\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image not found at {image_path}\")\n",
    "        return None, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"\"\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"1: Image-based input\")\n",
    "        print(\"2: Text-based input\")\n",
    "        print(\"3: Both image and text-based input\")\n",
    "        print(\"4: Exit\")\n",
    "\n",
    "        try:\n",
    "            choice = int(input(\"Select an option (1-4): \"))\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number between 1 and 4.\")\n",
    "            continue\n",
    "\n",
    "        match choice:\n",
    "            case 1:  # Image-based input\n",
    "                image_path = input(\"Enter the full path to the image file you want to predict: \")\n",
    "\n",
    "                if not os.path.exists(image_path):\n",
    "                    print(f\"Error: Image file not found at {image_path}\")\n",
    "                elif not image_path.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
    "                    print(\"Error: Please enter a valid JPEG, JPG, or PNG image file.\")\n",
    "                else:\n",
    "                    confidence, predicted_class, filename_base, image = predict_single_image(image_path)\n",
    "                    if confidence is not None:\n",
    "                        print(\"\\n--- Prediction Result ---\")\n",
    "                        print(f\"Filename: {filename_base}\")\n",
    "                        print(f\"Predicted Class: {predicted_class}\")\n",
    "                        print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "                        # Display the image using Matplotlib\n",
    "                        plt.figure()\n",
    "                        plt.imshow(image)\n",
    "                        plt.title(f\"Filename: {filename_base}\\nPredicted: {predicted_class} (Conf: {confidence:.2f})\")\n",
    "                        plt.axis('off')  # Turn off axis numbers and ticks\n",
    "                        plt.show()\n",
    "\n",
    "                        query = f\"{predicted_class}, {filename_base}\"\n",
    "\n",
    "            case 2:  # Text-based input\n",
    "                query = input(\"Ask a question about the medical documents (or type 'exit' to quit): \")\n",
    "\n",
    "            case 3:  # Both image and text-based input\n",
    "                image_path = input(\"Enter the full path to the image file you want to predict: \")\n",
    "\n",
    "                if not os.path.exists(image_path):\n",
    "                    print(f\"Error: Image file not found at {image_path}\")\n",
    "                elif not image_path.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
    "                    print(\"Error: Please enter a valid JPEG, JPG, or PNG image file.\")\n",
    "                else:\n",
    "                    confidence, predicted_class, filename_base, image = predict_single_image(image_path)\n",
    "                    if confidence is not None:\n",
    "                        print(\"\\n--- Prediction Result ---\")\n",
    "                        print(f\"Filename: {filename_base}\")\n",
    "                        print(f\"Predicted Class: {predicted_class}\")\n",
    "                        print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "                        # Display the image using Matplotlib\n",
    "                        plt.figure()\n",
    "                        plt.imshow(image)\n",
    "                        plt.title(f\"Filename: {filename_base}\\nPredicted: {predicted_class} (Conf: {confidence:.2f})\")\n",
    "                        plt.axis('off')  # Turn off axis numbers and ticks\n",
    "                        plt.show()\n",
    "\n",
    "                text_query = input(\"Ask a question about the medical documents (or type 'exit' to quit): \")\n",
    "                query = f\"{predicted_class}, {filename_base}, {text_query}\"\n",
    "\n",
    "            case 4:  # Exit\n",
    "                print(\"Exiting the question-answering session.\")\n",
    "                break\n",
    "\n",
    "            case _:  # Default case for invalid inputs\n",
    "                print(\"Invalid selection. Please select a valid option.\")\n",
    "\n",
    "        if query:\n",
    "            response = rag_chain.invoke({\"input\": query})\n",
    "            print(\"RAG Response:\", response[\"answer\"])\n",
    "        \n",
    "        text_to_speech_with_gtts(input_text=response[\"answer\"], output_filepath=\"gtts_output_file.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
